{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "# import feather\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import gc\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def append_to_csv(batch, csv_file):\n",
    "    props = dict(encoding='utf-8', index=False)\n",
    "    if not os.path.exists(csv_file):\n",
    "        batch.to_csv(csv_file, **props)\n",
    "    else:\n",
    "        batch.to_csv(csv_file, mode='a', header=False, **props)\n",
    "\n",
    "def delete_file_if_exists(filename):\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "fn = '../cache/bkup/train10_t_store{}.csv'.format(1)\n",
    "df =pd.read_csv(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df.head(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df[df.onpromotion == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fn = '../cache/bkup/train10_t_store{}.csv'.format(2)\n",
    "# df2 =pd.read_csv(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>item_family</th>\n",
       "      <th>item_class</th>\n",
       "      <th>perishable</th>\n",
       "      <th>city</th>\n",
       "      <th>...</th>\n",
       "      <th>dow</th>\n",
       "      <th>doy</th>\n",
       "      <th>dcount</th>\n",
       "      <th>h_type</th>\n",
       "      <th>h_desc</th>\n",
       "      <th>pd</th>\n",
       "      <th>wbe</th>\n",
       "      <th>wae</th>\n",
       "      <th>wfe</th>\n",
       "      <th>store_item_nbr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, date, store_nbr, item_nbr, unit_sales, onpromotion, item_family, item_class, perishable, city, state, store_type, cluster, dcoilwtico, transactions, dom, mon, dow, doy, dcount, h_type, h_desc, pd, wbe, wae, wfe, store_item_nbr]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 27 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df.onpromotion == True) & (df.store_item_nbr == '1_105737')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# len(set(df2.store_item_nbr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df2[df2.onpromotion == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df2.h_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df2.h_desc.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def days_from_next_promotion(store_num):\n",
    "    fn = '../cache/bkup/train10_t_store{}.csv'.format(store_num)\n",
    "    df =pd.read_csv(fn)\n",
    "    df['dts'] = df.date.map(np.datetime64)\n",
    "    df_min_date = np.min(df.dts)\n",
    "    df_max_date = np.min(df.dts)\n",
    "    max_days = ((df_max_date - df_min_date))/np.timedelta64(1, 'D')\n",
    "    n = 0\n",
    "    items = list(set(df.store_item_nbr))\n",
    "    for item in items:\n",
    "        n = 1\n",
    "        df_it = df[(df.store_item_nbr == item)].reset_index(drop=True)\n",
    "        \n",
    "        df_it['pe'] = 0 # promotion event\n",
    "        df_it['pe'][df_it.onpromotion == True] = 1\n",
    "        promo_indices = list(np.transpose(*np.where(df_it.pe)))\n",
    "        \n",
    "        df_it['days_from_next_promo'] = max_days\n",
    "        last_loc = 0\n",
    "        for j in promo_indices:\n",
    "            promo_date = df_it.loc[j, 'dts']\n",
    "            while last_loc < j:\n",
    "                curr_date = df_it.loc[last_loc, 'dts']\n",
    "                if(curr_date == promo_date):\n",
    "                    df_it.loc[last_loc, 'days_from_next_promo'] = 0\n",
    "                    continue\n",
    "                diff = int((promo_date - curr_date)/ np.timedelta64(1, 'D'))\n",
    "                df_it.loc[last_loc, 'days_from_next_promo'] = diff\n",
    "                last_loc += 1    \n",
    "            if last_loc == j:\n",
    "                df_it.loc[last_loc, 'days_from_next_promo'] = 0\n",
    "                last_loc += 1\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_next_promo_tmp.csv'.format(store_num, item)\n",
    "        delete_file_if_exists(fn_tmp)\n",
    "        df1 = pd.DataFrame()\n",
    "        df1['id'] = df_it['id'].values\n",
    "        df1['days_from_next_promo'] = df_it['days_from_next_promo'].values\n",
    "        df1.to_csv(fn_tmp, float_format='%.5f', index=False)\n",
    "        del df1\n",
    "    df2 = pd.DataFrame()\n",
    "    for item in items:\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_next_promo_tmp.csv'.format(store_num, item)\n",
    "        df1 = pd.read_csv(fn_tmp)\n",
    "        df2 = pd.concat([df2, df1], axis=0)\n",
    "    df2 = df2.sort('id')\n",
    "    df = df[(df.dts >= np.datetime64('2015-01-01')) & (df.dts <= np.datetime64('2017-08-15'))]\n",
    "    min_id = df.iloc[0]['id']\n",
    "    df2 = df2[df2.id >= min_id]\n",
    "    assert(len(df) == len(df2))\n",
    "    # assert(df.id == df2.id)\n",
    "    s = sum(df2.isnull().any()==True) # print NA\n",
    "    print(s)\n",
    "    if s > 0:\n",
    "        print(df2.isnull().sum())\n",
    "    fn_tmp = '../cache/train10_t_store{}_days_from_next_promo.csv'.format(store_num)\n",
    "    delete_file_if_exists(fn_tmp)\n",
    "    df2.to_csv(fn_tmp, float_format='%.5f', index=False)\n",
    "    del df2\n",
    "    del df\n",
    "    for item in items:\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_next_promo_tmp.csv'.format(store_num, item)\n",
    "        delete_file_if_exists(fn_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def days_from_prev_promotion(store_num):\n",
    "    fn = '../cache/bkup/train10_t_store{}.csv'.format(store_num)\n",
    "    df =pd.read_csv(fn)\n",
    "    df['dts'] = df.date.map(np.datetime64)\n",
    "    df_min_date = np.min(df.dts)\n",
    "    df_max_date = np.min(df.dts)\n",
    "    max_days = ((df_max_date - df_min_date))/np.timedelta64(1, 'D')\n",
    "    n = 0\n",
    "    items = list(set(df.store_item_nbr))\n",
    "    for item in items:\n",
    "        n = 1\n",
    "        df_it = df[(df.store_item_nbr == item)].reset_index(drop=True)\n",
    "        \n",
    "        df_it['pe'] = 0 # promotion event\n",
    "        df_it['pe'][df_it.onpromotion == True] = 1\n",
    "        promo_indices = list(np.transpose(*np.where(df_it.pe)))\n",
    "        \n",
    "        df_it['days_from_prev_promo'] = max_days\n",
    "        last_loc = len(df_it)-1\n",
    "        for j in promo_indices:\n",
    "            promo_date = df_it.loc[j, 'dts']\n",
    "            while last_loc > j:\n",
    "                curr_date = df_it.loc[last_loc, 'dts']\n",
    "                if(curr_date == promo_date):\n",
    "                    df_it.loc[last_loc, 'days_from_prev_promo'] = 0\n",
    "                    continue\n",
    "                diff = int((promo_date - curr_date)/ np.timedelta64(1, 'D'))\n",
    "                df_it.loc[last_loc, 'days_from_prev_promo'] = abs(diff)\n",
    "                last_loc -= 1    \n",
    "            if last_loc == j:\n",
    "                df_it.loc[last_loc, 'days_from_prev_promo'] = 0\n",
    "                last_loc -= 1\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_prev_promo_tmp.csv'.format(store_num, item)\n",
    "        delete_file_if_exists(fn_tmp)\n",
    "        df1 = pd.DataFrame()\n",
    "        df1['id'] = df_it['id'].values\n",
    "        df1['days_from_prev_promo'] = df_it['days_from_prev_promo'].values\n",
    "        df1.to_csv(fn_tmp, float_format='%.5f', index=False)\n",
    "        del df1\n",
    "    df2 = pd.DataFrame()\n",
    "    for item in items:\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_prev_promo_tmp.csv'.format(store_num, item)\n",
    "        df1 = pd.read_csv(fn_tmp)\n",
    "        df2 = pd.concat([df2, df1], axis=0)\n",
    "    df2 = df2.sort('id')\n",
    "    df = df[(df.dts >= np.datetime64('2015-01-01')) & (df.dts <= np.datetime64('2017-08-15'))]\n",
    "    min_id = df.iloc[0]['id']\n",
    "    df2 = df2[df2.id >= min_id]\n",
    "    assert(len(df) == len(df2))\n",
    "    # assert(df.id == df2.id)\n",
    "    s = sum(df2.isnull().any()==True) # print NA\n",
    "    print(s)\n",
    "    if s > 0:\n",
    "        print(df2.isnull().sum())\n",
    "    fn_tmp = '../cache/train10_t_store{}_days_from_prev_promo.csv'.format(store_num)\n",
    "    delete_file_if_exists(fn_tmp)\n",
    "    df2.to_csv(fn_tmp, float_format='%.5f', index=False)\n",
    "    del df2\n",
    "    del df\n",
    "    for item in items:\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_prev_promo_tmp.csv'.format(store_num, item)\n",
    "        delete_file_if_exists(fn_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def days_from_next_event(store_num):\n",
    "    fn = '../cache/bkup/train10_t_store{}.csv'.format(store_num)\n",
    "    df =pd.read_csv(fn)\n",
    "    df['dts'] = df.date.map(np.datetime64)\n",
    "    df_min_date = np.min(df.dts)\n",
    "    df_max_date = np.min(df.dts)\n",
    "    max_days = ((df_max_date - df_min_date))/np.timedelta64(1, 'D')\n",
    "    n = 0\n",
    "    items = list(set(df.store_item_nbr))\n",
    "    for item in items:\n",
    "        n = 1\n",
    "        df_it = df[(df.store_item_nbr == item)].reset_index(drop=True)\n",
    "        \n",
    "        df_it['pe'] = 0 # promotion event\n",
    "        df_it['pe'][df_it.h_type == 'Event'] = 1\n",
    "        promo_indices = list(np.transpose(*np.where(df_it.pe)))\n",
    "        \n",
    "        df_it['days_from_next_event'] = max_days\n",
    "        last_loc = 0\n",
    "        for j in promo_indices:\n",
    "            promo_date = df_it.loc[j, 'dts']\n",
    "            while last_loc < j:\n",
    "                curr_date = df_it.loc[last_loc, 'dts']\n",
    "                if(curr_date == promo_date):\n",
    "                    df_it.loc[last_loc, 'days_from_next_event'] = 0\n",
    "                    continue\n",
    "                diff = int((promo_date - curr_date)/ np.timedelta64(1, 'D'))\n",
    "                df_it.loc[last_loc, 'days_from_next_event'] = diff\n",
    "                last_loc += 1    \n",
    "            if last_loc == j:\n",
    "                df_it.loc[last_loc, 'days_from_next_event'] = 0\n",
    "                last_loc += 1\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_next_event_tmp.csv'.format(store_num, item)\n",
    "        delete_file_if_exists(fn_tmp)\n",
    "        df1 = pd.DataFrame()\n",
    "        df1['id'] = df_it['id'].values\n",
    "        df1['days_from_next_event'] = df_it['days_from_next_event'].values\n",
    "        df1.to_csv(fn_tmp, float_format='%.5f', index=False)\n",
    "        del df1\n",
    "    df2 = pd.DataFrame()\n",
    "    for item in items:\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_next_event_tmp.csv'.format(store_num, item)\n",
    "        df1 = pd.read_csv(fn_tmp)\n",
    "        df2 = pd.concat([df2, df1], axis=0)\n",
    "    df2 = df2.sort('id')\n",
    "    df = df[(df.dts >= np.datetime64('2015-01-01')) & (df.dts <= np.datetime64('2017-08-15'))]\n",
    "    min_id = df.iloc[0]['id']\n",
    "    df2 = df2[df2.id >= min_id]\n",
    "    assert(len(df) == len(df2))\n",
    "    # assert(df.id == df2.id)\n",
    "    s = sum(df2.isnull().any()==True) # print NA\n",
    "    print(s)\n",
    "    if s > 0:\n",
    "        print(df2.isnull().sum())\n",
    "    fn_tmp = '../cache/train10_t_store{}_days_from_next_event.csv'.format(store_num)\n",
    "    delete_file_if_exists(fn_tmp)\n",
    "    df2.to_csv(fn_tmp, float_format='%.5f', index=False)\n",
    "    del df2\n",
    "    del df\n",
    "    for item in items:\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_next_event_tmp.csv'.format(store_num, item)\n",
    "        delete_file_if_exists(fn_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def days_from_prev_event(store_num):\n",
    "    fn = '../cache/bkup/train10_t_store{}.csv'.format(store_num)\n",
    "    df =pd.read_csv(fn)\n",
    "    df['dts'] = df.date.map(np.datetime64)\n",
    "    df_min_date = np.min(df.dts)\n",
    "    df_max_date = np.min(df.dts)\n",
    "    max_days = ((df_max_date - df_min_date))/np.timedelta64(1, 'D')\n",
    "    n = 0\n",
    "    items = list(set(df.store_item_nbr))\n",
    "    for item in items:\n",
    "        n = 1\n",
    "        df_it = df[(df.store_item_nbr == item)].reset_index(drop=True)\n",
    "        \n",
    "        df_it['pe'] = 0 # promotion event\n",
    "        df_it['pe'][df_it.h_type == 'Event'] = 1\n",
    "        promo_indices = list(np.transpose(*np.where(df_it.pe)))\n",
    "        \n",
    "        df_it['days_from_prev_event'] = max_days\n",
    "        last_loc = len(df_it)-1\n",
    "        for j in promo_indices:\n",
    "            promo_date = df_it.loc[j, 'dts']\n",
    "            while last_loc > j:\n",
    "                curr_date = df_it.loc[last_loc, 'dts']\n",
    "                if(curr_date == promo_date):\n",
    "                    df_it.loc[last_loc, 'days_from_prev_event'] = 0\n",
    "                    continue\n",
    "                diff = int((promo_date - curr_date)/ np.timedelta64(1, 'D'))\n",
    "                df_it.loc[last_loc, 'days_from_prev_event'] = abs(diff)\n",
    "                last_loc -= 1    \n",
    "            if last_loc == j:\n",
    "                df_it.loc[last_loc, 'days_from_prev_event'] = 0\n",
    "                last_loc -= 1\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_prev_event_tmp.csv'.format(store_num, item)\n",
    "        delete_file_if_exists(fn_tmp)\n",
    "        df1 = pd.DataFrame()\n",
    "        df1['id'] = df_it['id'].values\n",
    "        df1['days_from_prev_event'] = df_it['days_from_prev_event'].values\n",
    "        df1.to_csv(fn_tmp, float_format='%.5f', index=False)\n",
    "        del df1\n",
    "    df2 = pd.DataFrame()\n",
    "    for item in items:\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_prev_event_tmp.csv'.format(store_num, item)\n",
    "        df1 = pd.read_csv(fn_tmp)\n",
    "        df2 = pd.concat([df2, df1], axis=0)\n",
    "    df2 = df2.sort('id')\n",
    "    df = df[(df.dts >= np.datetime64('2015-01-01')) & (df.dts <= np.datetime64('2017-08-15'))]\n",
    "    min_id = df.iloc[0]['id']\n",
    "    df2 = df2[df2.id >= min_id]\n",
    "    assert(len(df) == len(df2))\n",
    "    # assert(df.id == df2.id)\n",
    "    s = sum(df2.isnull().any()==True) # print NA\n",
    "    print(s)\n",
    "    if s > 0:\n",
    "        print(df2.isnull().sum())\n",
    "    fn_tmp = '../cache/train10_t_store{}_days_from_prev_event.csv'.format(store_num)\n",
    "    delete_file_if_exists(fn_tmp)\n",
    "    df2.to_csv(fn_tmp, float_format='%.5f', index=False)\n",
    "    del df2\n",
    "    del df\n",
    "    for item in items:\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_prev_event_tmp.csv'.format(store_num, item)\n",
    "        delete_file_if_exists(fn_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def days_from_next_holiday(store_num):\n",
    "    fn = '../cache/bkup/train10_t_store{}.csv'.format(store_num)\n",
    "    df =pd.read_csv(fn)\n",
    "    df['dts'] = df.date.map(np.datetime64)\n",
    "    df_min_date = np.min(df.dts)\n",
    "    df_max_date = np.min(df.dts)\n",
    "    max_days = ((df_max_date - df_min_date))/np.timedelta64(1, 'D')\n",
    "    n = 0\n",
    "    items = list(set(df.store_item_nbr))\n",
    "    for item in items:\n",
    "        n = 1\n",
    "        df_it = df[(df.store_item_nbr == item)].reset_index(drop=True)\n",
    "        \n",
    "        df_it['pe'] = 0 # promotion event\n",
    "        df_it['pe'][df_it.h_type == 'Holiday'] = 1\n",
    "        promo_indices = list(np.transpose(*np.where(df_it.pe)))\n",
    "        \n",
    "        df_it['days_from_next_holiday'] = max_days\n",
    "        last_loc = 0\n",
    "        for j in promo_indices:\n",
    "            promo_date = df_it.loc[j, 'dts']\n",
    "            while last_loc < j:\n",
    "                curr_date = df_it.loc[last_loc, 'dts']\n",
    "                if(curr_date == promo_date):\n",
    "                    df_it.loc[last_loc, 'days_from_next_holiday'] = 0\n",
    "                    continue\n",
    "                diff = int((promo_date - curr_date)/ np.timedelta64(1, 'D'))\n",
    "                df_it.loc[last_loc, 'days_from_next_holiday'] = diff\n",
    "                last_loc += 1    \n",
    "            if last_loc == j:\n",
    "                df_it.loc[last_loc, 'days_from_next_holiday'] = 0\n",
    "                last_loc += 1\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_next_holiday_tmp.csv'.format(store_num, item)\n",
    "        delete_file_if_exists(fn_tmp)\n",
    "        df1 = pd.DataFrame()\n",
    "        df1['id'] = df_it['id'].values\n",
    "        df1['days_from_next_holiday'] = df_it['days_from_next_holiday'].values\n",
    "        df1.to_csv(fn_tmp, float_format='%.5f', index=False)\n",
    "        del df1\n",
    "    df2 = pd.DataFrame()\n",
    "    for item in items:\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_next_holiday_tmp.csv'.format(store_num, item)\n",
    "        df1 = pd.read_csv(fn_tmp)\n",
    "        df2 = pd.concat([df2, df1], axis=0)\n",
    "    df2 = df2.sort('id')\n",
    "    df = df[(df.dts >= np.datetime64('2015-01-01')) & (df.dts <= np.datetime64('2017-08-15'))]\n",
    "    min_id = df.iloc[0]['id']\n",
    "    df2 = df2[df2.id >= min_id]\n",
    "    assert(len(df) == len(df2))\n",
    "    # assert(df.id == df2.id)\n",
    "    s = sum(df2.isnull().any()==True) # print NA\n",
    "    print(s)\n",
    "    if s > 0:\n",
    "        print(df2.isnull().sum())\n",
    "    fn_tmp = '../cache/train10_t_store{}_days_from_next_holiday.csv'.format(store_num)\n",
    "    delete_file_if_exists(fn_tmp)\n",
    "    df2.to_csv(fn_tmp, float_format='%.5f', index=False)\n",
    "    del df2\n",
    "    del df\n",
    "    for item in items:\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_next_holiday_tmp.csv'.format(store_num, item)\n",
    "        delete_file_if_exists(fn_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def days_from_prev_holiday(store_num):\n",
    "    fn = '../cache/bkup/train10_t_store{}.csv'.format(store_num)\n",
    "    df =pd.read_csv(fn)\n",
    "    df['dts'] = df.date.map(np.datetime64)\n",
    "    df_min_date = np.min(df.dts)\n",
    "    df_max_date = np.min(df.dts)\n",
    "    max_days = ((df_max_date - df_min_date))/np.timedelta64(1, 'D')\n",
    "    n = 0\n",
    "    items = list(set(df.store_item_nbr))\n",
    "    for item in items:\n",
    "        n = 1\n",
    "        df_it = df[(df.store_item_nbr == item)].reset_index(drop=True)\n",
    "        \n",
    "        df_it['pe'] = 0 # promotion event\n",
    "        df_it['pe'][df_it.h_type == 'Holiday'] = 1\n",
    "        promo_indices = list(np.transpose(*np.where(df_it.pe)))\n",
    "        \n",
    "        df_it['days_from_prev_holiday'] = max_days\n",
    "        last_loc = len(df_it)-1\n",
    "        for j in promo_indices:\n",
    "            promo_date = df_it.loc[j, 'dts']\n",
    "            while last_loc > j:\n",
    "                curr_date = df_it.loc[last_loc, 'dts']\n",
    "                if(curr_date == promo_date):\n",
    "                    df_it.loc[last_loc, 'days_from_prev_holiday'] = 0\n",
    "                    continue\n",
    "                diff = int((promo_date - curr_date)/ np.timedelta64(1, 'D'))\n",
    "                df_it.loc[last_loc, 'days_from_prev_holiday'] = abs(diff)\n",
    "                last_loc -= 1    \n",
    "            if last_loc == j:\n",
    "                df_it.loc[last_loc, 'days_from_prev_holiday'] = 0\n",
    "                last_loc -= 1\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_prev_holiday_tmp.csv'.format(store_num, item)\n",
    "        delete_file_if_exists(fn_tmp)\n",
    "        df1 = pd.DataFrame()\n",
    "        df1['id'] = df_it['id'].values\n",
    "        df1['days_from_prev_holiday'] = df_it['days_from_prev_holiday'].values\n",
    "        df1.to_csv(fn_tmp, float_format='%.5f', index=False)\n",
    "        del df1\n",
    "    df2 = pd.DataFrame()\n",
    "    for item in items:\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_prev_holiday_tmp.csv'.format(store_num, item)\n",
    "        df1 = pd.read_csv(fn_tmp)\n",
    "        df2 = pd.concat([df2, df1], axis=0)\n",
    "    df2 = df2.sort('id')\n",
    "    df = df[(df.dts >= np.datetime64('2015-01-01')) & (df.dts <= np.datetime64('2017-08-15'))]\n",
    "    min_id = df.iloc[0]['id']\n",
    "    df2 = df2[df2.id >= min_id]\n",
    "    assert(len(df) == len(df2))\n",
    "    # assert(df.id == df2.id)\n",
    "    s = sum(df2.isnull().any()==True) # print NA\n",
    "    print(s)\n",
    "    if s > 0:\n",
    "        print(df2.isnull().sum())\n",
    "    fn_tmp = '../cache/train10_t_store{}_days_from_prev_holiday.csv'.format(store_num)\n",
    "    delete_file_if_exists(fn_tmp)\n",
    "    df2.to_csv(fn_tmp, float_format='%.5f', index=False)\n",
    "    del df2\n",
    "    del df\n",
    "    for item in items:\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_prev_holiday_tmp.csv'.format(store_num, item)\n",
    "        delete_file_if_exists(fn_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def days_from_next_nw(store_num):\n",
    "    fn = '../cache/bkup/train10_t_store{}.csv'.format(store_num)\n",
    "    df =pd.read_csv(fn)\n",
    "    df['dts'] = df.date.map(np.datetime64)\n",
    "    df_min_date = np.min(df.dts)\n",
    "    df_max_date = np.min(df.dts)\n",
    "    max_days = ((df_max_date - df_min_date))/np.timedelta64(1, 'D')\n",
    "    n = 0\n",
    "    items = list(set(df.store_item_nbr))\n",
    "    for item in items:\n",
    "        n = 1\n",
    "        df_it = df[(df.store_item_nbr == item)].reset_index(drop=True)\n",
    "        \n",
    "        df_it['pe'] = 0 # promotion event\n",
    "        df_it['pe'][df_it.h_type != 'Work Day'] = 1\n",
    "        promo_indices = list(np.transpose(*np.where(df_it.pe)))\n",
    "        \n",
    "        df_it['days_from_next_nw'] = max_days\n",
    "        last_loc = 0\n",
    "        for j in promo_indices:\n",
    "            promo_date = df_it.loc[j, 'dts']\n",
    "            while last_loc < j:\n",
    "                curr_date = df_it.loc[last_loc, 'dts']\n",
    "                if(curr_date == promo_date):\n",
    "                    df_it.loc[last_loc, 'days_from_next_nw'] = 0\n",
    "                    continue\n",
    "                diff = int((promo_date - curr_date)/ np.timedelta64(1, 'D'))\n",
    "                df_it.loc[last_loc, 'days_from_next_nw'] = diff\n",
    "                last_loc += 1    \n",
    "            if last_loc == j:\n",
    "                df_it.loc[last_loc, 'days_from_next_nw'] = 0\n",
    "                last_loc += 1\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_next_nw_tmp.csv'.format(store_num, item)\n",
    "        delete_file_if_exists(fn_tmp)\n",
    "        df1 = pd.DataFrame()\n",
    "        df1['id'] = df_it['id'].values\n",
    "        df1['days_from_next_nw'] = df_it['days_from_next_nw'].values\n",
    "        df1.to_csv(fn_tmp, float_format='%.5f', index=False)\n",
    "        del df1\n",
    "    df2 = pd.DataFrame()\n",
    "    for item in items:\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_next_nw_tmp.csv'.format(store_num, item)\n",
    "        df1 = pd.read_csv(fn_tmp)\n",
    "        df2 = pd.concat([df2, df1], axis=0)\n",
    "    df2 = df2.sort('id')\n",
    "    df = df[(df.dts >= np.datetime64('2015-01-01')) & (df.dts <= np.datetime64('2017-08-15'))]\n",
    "    min_id = df.iloc[0]['id']\n",
    "    df2 = df2[df2.id >= min_id]\n",
    "    assert(len(df) == len(df2))\n",
    "    # assert(df.id == df2.id)\n",
    "    s = sum(df2.isnull().any()==True) # print NA\n",
    "    print(s)\n",
    "    if s > 0:\n",
    "        print(df2.isnull().sum())\n",
    "    fn_tmp = '../cache/train10_t_store{}_days_from_next_nw.csv'.format(store_num)\n",
    "    delete_file_if_exists(fn_tmp)\n",
    "    df2.to_csv(fn_tmp, float_format='%.5f', index=False)\n",
    "    del df2\n",
    "    del df\n",
    "    for item in items:\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_next_nw_tmp.csv'.format(store_num, item)\n",
    "        delete_file_if_exists(fn_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def days_from_prev_nw(store_num):\n",
    "    fn = '../cache/bkup/train10_t_store{}.csv'.format(store_num)\n",
    "    df =pd.read_csv(fn)\n",
    "    df['dts'] = df.date.map(np.datetime64)\n",
    "    df_min_date = np.min(df.dts)\n",
    "    df_max_date = np.min(df.dts)\n",
    "    max_days = ((df_max_date - df_min_date))/np.timedelta64(1, 'D')\n",
    "    n = 0\n",
    "    items = list(set(df.store_item_nbr))\n",
    "    for item in items:\n",
    "        n = 1\n",
    "        df_it = df[(df.store_item_nbr == item)].reset_index(drop=True)\n",
    "        \n",
    "        df_it['pe'] = 0 # promotion event\n",
    "        df_it['pe'][df_it.h_type != 'Work Day'] = 1\n",
    "        promo_indices = list(np.transpose(*np.where(df_it.pe)))\n",
    "        \n",
    "        df_it['days_from_prev_nw'] = max_days\n",
    "        last_loc = len(df_it)-1\n",
    "        for j in promo_indices:\n",
    "            promo_date = df_it.loc[j, 'dts']\n",
    "            while last_loc > j:\n",
    "                curr_date = df_it.loc[last_loc, 'dts']\n",
    "                if(curr_date == promo_date):\n",
    "                    df_it.loc[last_loc, 'days_from_prev_nw'] = 0\n",
    "                    continue\n",
    "                diff = int((promo_date - curr_date)/ np.timedelta64(1, 'D'))\n",
    "                df_it.loc[last_loc, 'days_from_prev_nw'] = abs(diff)\n",
    "                last_loc -= 1    \n",
    "            if last_loc == j:\n",
    "                df_it.loc[last_loc, 'days_from_prev_nw'] = 0\n",
    "                last_loc -= 1\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_prev_nw_tmp.csv'.format(store_num, item)\n",
    "        delete_file_if_exists(fn_tmp)\n",
    "        df1 = pd.DataFrame()\n",
    "        df1['id'] = df_it['id'].values\n",
    "        df1['days_from_prev_nw'] = df_it['days_from_prev_nw'].values\n",
    "        df1.to_csv(fn_tmp, float_format='%.5f', index=False)\n",
    "        del df1\n",
    "    df2 = pd.DataFrame()\n",
    "    for item in items:\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_prev_nw_tmp.csv'.format(store_num, item)\n",
    "        df1 = pd.read_csv(fn_tmp)\n",
    "        df2 = pd.concat([df2, df1], axis=0)\n",
    "    df2 = df2.sort('id')\n",
    "    df = df[(df.dts >= np.datetime64('2015-01-01')) & (df.dts <= np.datetime64('2017-08-15'))]\n",
    "    min_id = df.iloc[0]['id']\n",
    "    df2 = df2[df2.id >= min_id]\n",
    "    assert(len(df) == len(df2))\n",
    "    # assert(df.id == df2.id)\n",
    "    s = sum(df2.isnull().any()==True) # print NA\n",
    "    print(s)\n",
    "    if s > 0:\n",
    "        print(df2.isnull().sum())\n",
    "    fn_tmp = '../cache/train10_t_store{}_days_from_prev_nw.csv'.format(store_num)\n",
    "    delete_file_if_exists(fn_tmp)\n",
    "    df2.to_csv(fn_tmp, float_format='%.5f', index=False)\n",
    "    del df2\n",
    "    del df\n",
    "    for item in items:\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_prev_nw_tmp.csv'.format(store_num, item)\n",
    "        delete_file_if_exists(fn_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def days_from_next_nn(store_num):\n",
    "    fn = '../cache/bkup/train10_t_store{}.csv'.format(store_num)\n",
    "    df =pd.read_csv(fn)\n",
    "    df['dts'] = df.date.map(np.datetime64)\n",
    "    df_min_date = np.min(df.dts)\n",
    "    df_max_date = np.min(df.dts)\n",
    "    max_days = ((df_max_date - df_min_date))/np.timedelta64(1, 'D')\n",
    "    n = 0\n",
    "    items = list(set(df.store_item_nbr))\n",
    "    for item in items:\n",
    "        n = 1\n",
    "        df_it = df[(df.store_item_nbr == item)].reset_index(drop=True)\n",
    "        \n",
    "        df_it['pe'] = 0 # promotion event\n",
    "        df_it['pe'][df_it.h_desc != 'Normal'] = 1\n",
    "        promo_indices = list(np.transpose(*np.where(df_it.pe)))\n",
    "        \n",
    "        df_it['days_from_next_nn'] = max_days\n",
    "        last_loc = 0\n",
    "        for j in promo_indices:\n",
    "            promo_date = df_it.loc[j, 'dts']\n",
    "            while last_loc < j:\n",
    "                curr_date = df_it.loc[last_loc, 'dts']\n",
    "                if(curr_date == promo_date):\n",
    "                    df_it.loc[last_loc, 'days_from_next_nn'] = 0\n",
    "                    continue\n",
    "                diff = int((promo_date - curr_date)/ np.timedelta64(1, 'D'))\n",
    "                df_it.loc[last_loc, 'days_from_next_nn'] = diff\n",
    "                last_loc += 1    \n",
    "            if last_loc == j:\n",
    "                df_it.loc[last_loc, 'days_from_next_nn'] = 0\n",
    "                last_loc += 1\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_next_nn_tmp.csv'.format(store_num, item)\n",
    "        delete_file_if_exists(fn_tmp)\n",
    "        df1 = pd.DataFrame()\n",
    "        df1['id'] = df_it['id'].values\n",
    "        df1['days_from_next_nn'] = df_it['days_from_next_nn'].values\n",
    "        df1.to_csv(fn_tmp, float_format='%.5f', index=False)\n",
    "        del df1\n",
    "    df2 = pd.DataFrame()\n",
    "    for item in items:\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_next_nn_tmp.csv'.format(store_num, item)\n",
    "        df1 = pd.read_csv(fn_tmp)\n",
    "        df2 = pd.concat([df2, df1], axis=0)\n",
    "    df2 = df2.sort('id')\n",
    "    df = df[(df.dts >= np.datetime64('2015-01-01')) & (df.dts <= np.datetime64('2017-08-15'))]\n",
    "    min_id = df.iloc[0]['id']\n",
    "    df2 = df2[df2.id >= min_id]\n",
    "    assert(len(df) == len(df2))\n",
    "    # assert(df.id == df2.id)\n",
    "    s = sum(df2.isnull().any()==True) # print NA\n",
    "    print(s)\n",
    "    if s > 0:\n",
    "        print(df2.isnull().sum())\n",
    "    fn_tmp = '../cache/train10_t_store{}_days_from_next_nn.csv'.format(store_num)\n",
    "    delete_file_if_exists(fn_tmp)\n",
    "    df2.to_csv(fn_tmp, float_format='%.5f', index=False)\n",
    "    del df2\n",
    "    del df\n",
    "    for item in items:\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_next_nn_tmp.csv'.format(store_num, item)\n",
    "        delete_file_if_exists(fn_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def days_from_prev_nn(store_num):\n",
    "    fn = '../cache/bkup/train10_t_store{}.csv'.format(store_num)\n",
    "    df =pd.read_csv(fn)\n",
    "    df['dts'] = df.date.map(np.datetime64)\n",
    "    df_min_date = np.min(df.dts)\n",
    "    df_max_date = np.min(df.dts)\n",
    "    max_days = ((df_max_date - df_min_date))/np.timedelta64(1, 'D')\n",
    "    n = 0\n",
    "    items = list(set(df.store_item_nbr))\n",
    "    for item in items:\n",
    "        n = 1\n",
    "        df_it = df[(df.store_item_nbr == item)].reset_index(drop=True)\n",
    "        \n",
    "        df_it['pe'] = 0 # promotion event\n",
    "        df_it['pe'][df_it.h_desc != 'Normal'] = 1\n",
    "        promo_indices = list(np.transpose(*np.where(df_it.pe)))\n",
    "        \n",
    "        df_it['days_from_prev_nn'] = max_days\n",
    "        last_loc = len(df_it)-1\n",
    "        for j in promo_indices:\n",
    "            promo_date = df_it.loc[j, 'dts']\n",
    "            while last_loc > j:\n",
    "                curr_date = df_it.loc[last_loc, 'dts']\n",
    "                if(curr_date == promo_date):\n",
    "                    df_it.loc[last_loc, 'days_from_prev_nn'] = 0\n",
    "                    continue\n",
    "                diff = int((promo_date - curr_date)/ np.timedelta64(1, 'D'))\n",
    "                df_it.loc[last_loc, 'days_from_prev_nn'] = abs(diff)\n",
    "                last_loc -= 1    \n",
    "            if last_loc == j:\n",
    "                df_it.loc[last_loc, 'days_from_prev_nn'] = 0\n",
    "                last_loc -= 1\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_prev_nn_tmp.csv'.format(store_num, item)\n",
    "        delete_file_if_exists(fn_tmp)\n",
    "        df1 = pd.DataFrame()\n",
    "        df1['id'] = df_it['id'].values\n",
    "        df1['days_from_prev_nn'] = df_it['days_from_prev_nn'].values\n",
    "        df1.to_csv(fn_tmp, float_format='%.5f', index=False)\n",
    "        del df1\n",
    "    df2 = pd.DataFrame()\n",
    "    for item in items:\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_prev_nn_tmp.csv'.format(store_num, item)\n",
    "        df1 = pd.read_csv(fn_tmp)\n",
    "        df2 = pd.concat([df2, df1], axis=0)\n",
    "    df2 = df2.sort('id')\n",
    "    df = df[(df.dts >= np.datetime64('2015-01-01')) & (df.dts <= np.datetime64('2017-08-15'))]\n",
    "    min_id = df.iloc[0]['id']\n",
    "    df2 = df2[df2.id >= min_id]\n",
    "    assert(len(df) == len(df2))\n",
    "    # assert(df.id == df2.id)\n",
    "    s = sum(df2.isnull().any()==True) # print NA\n",
    "    print(s)\n",
    "    if s > 0:\n",
    "        print(df2.isnull().sum())\n",
    "    fn_tmp = '../cache/train10_t_store{}_days_from_prev_nn.csv'.format(store_num)\n",
    "    delete_file_if_exists(fn_tmp)\n",
    "    df2.to_csv(fn_tmp, float_format='%.5f', index=False)\n",
    "    del df2\n",
    "    del df\n",
    "    for item in items:\n",
    "        fn_tmp = '../cache/train10_t_store{}_{}_prev_nn_tmp.csv'.format(store_num, item)\n",
    "        delete_file_if_exists(fn_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# days_from_next_promotion(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for i in tqdm(range(1,55)):\n",
    "#     days_from_next_promotion(i)\n",
    "#     days_from_prev_promotion(i)\n",
    "#     days_from_next_event(i)\n",
    "#     days_from_prev_event(i)\n",
    "#     days_from_next_holiday(i)\n",
    "#     days_from_prev_holiday(i)\n",
    "#     days_from_next_nw(i)\n",
    "#     days_from_prev_nw(i)\n",
    "#     days_from_next_nn(i)\n",
    "#     days_from_prev_nn(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def days_from_next_nw_test(store_num):\n",
    "    fn = '../cache/bkup/test7_t_store{}.csv'.format(store_num)\n",
    "    df =pd.read_csv(fn)\n",
    "    df['dts'] = df.date.map(np.datetime64)\n",
    "    df_min_date = np.min(df.dts)\n",
    "    df_max_date = np.min(df.dts)\n",
    "    max_days = ((df_max_date - df_min_date))/np.timedelta64(1, 'D')\n",
    "    n = 0\n",
    "    items = list(set(df.store_item_nbr))\n",
    "    for item in items:\n",
    "        n = 1\n",
    "        df_it = df[(df.store_item_nbr == item)].reset_index(drop=True)\n",
    "        \n",
    "        df_it['pe'] = 0 # promotion event\n",
    "        df_it['pe'][df_it.h_type != 'Work Day'] = 1\n",
    "        promo_indices = list(np.transpose(*np.where(df_it.pe)))\n",
    "        \n",
    "        df_it['days_from_next_nw'] = max_days\n",
    "        last_loc = 0\n",
    "        for j in promo_indices:\n",
    "            promo_date = df_it.loc[j, 'dts']\n",
    "            while last_loc < j:\n",
    "                curr_date = df_it.loc[last_loc, 'dts']\n",
    "                if(curr_date == promo_date):\n",
    "                    df_it.loc[last_loc, 'days_from_next_nw'] = 0\n",
    "                    continue\n",
    "                diff = int((promo_date - curr_date)/ np.timedelta64(1, 'D'))\n",
    "                df_it.loc[last_loc, 'days_from_next_nw'] = diff\n",
    "                last_loc += 1    \n",
    "            if last_loc == j:\n",
    "                df_it.loc[last_loc, 'days_from_next_nw'] = 0\n",
    "                last_loc += 1\n",
    "        fn_tmp = '../cache/test7_t_store{}_{}_next_nw_tmp.csv'.format(store_num, item)\n",
    "        delete_file_if_exists(fn_tmp)\n",
    "        df1 = pd.DataFrame()\n",
    "        df1['id'] = df_it['id'].values\n",
    "        df1['days_from_next_nw'] = df_it['days_from_next_nw'].values\n",
    "        df1.to_csv(fn_tmp, float_format='%.5f', index=False)\n",
    "        del df1\n",
    "    df2 = pd.DataFrame()\n",
    "    for item in items:\n",
    "        fn_tmp = '../cache/test7_t_store{}_{}_next_nw_tmp.csv'.format(store_num, item)\n",
    "        df1 = pd.read_csv(fn_tmp)\n",
    "        df2 = pd.concat([df2, df1], axis=0)\n",
    "    df2 = df2.sort('id')\n",
    "    df = df[(df.dts >= np.datetime64('2017-08-16')) & (df.dts <= np.datetime64('2017-08-31'))]\n",
    "    min_id = df.iloc[0]['id']\n",
    "    df2 = df2[df2.id >= min_id]\n",
    "    assert(len(df) == len(df2))\n",
    "    # assert(df.id == df2.id)\n",
    "    s = sum(df2.isnull().any()==True) # print NA\n",
    "    print(s)\n",
    "    if s > 0:\n",
    "        print(df2.isnull().sum())\n",
    "    fn_tmp = '../cache/test7_t_store{}_days_from_next_nw.csv'.format(store_num)\n",
    "    delete_file_if_exists(fn_tmp)\n",
    "    df2.to_csv(fn_tmp, float_format='%.5f', index=False)\n",
    "    del df2\n",
    "    del df\n",
    "    for item in items:\n",
    "        fn_tmp = '../cache/test7_t_store{}_{}_next_nw_tmp.csv'.format(store_num, item)\n",
    "        delete_file_if_exists(fn_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/54 [00:00<?, ?it/s]/opt/conda/lib/python3.6/site-packages/ipykernel/__main__.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel/__main__.py:45: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  2%|▏         | 1/54 [04:29<3:58:05, 269.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▎         | 2/54 [09:32<4:02:24, 279.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 3/54 [14:37<4:04:01, 287.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 4/54 [19:40<4:03:22, 292.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|▉         | 5/54 [24:49<4:02:27, 296.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 6/54 [29:52<3:59:04, 298.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 7/54 [34:55<3:55:07, 300.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▍        | 8/54 [40:00<3:51:13, 301.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 9/54 [45:10<3:48:02, 304.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|█▊        | 10/54 [50:16<3:43:24, 304.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 11/54 [55:23<3:38:47, 305.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|██▏       | 12/54 [1:00:29<3:33:52, 305.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██▍       | 13/54 [1:05:33<3:28:29, 305.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██▌       | 14/54 [1:10:35<3:22:45, 304.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|██▊       | 15/54 [1:15:35<3:16:54, 302.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|██▉       | 16/54 [1:20:35<3:11:22, 302.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███▏      | 17/54 [1:25:40<3:06:51, 303.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 18/54 [1:30:38<3:00:44, 301.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 19/54 [1:35:36<2:55:18, 300.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███▋      | 20/54 [1:40:35<2:49:57, 299.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███▉      | 21/54 [1:45:38<2:45:32, 300.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|████      | 22/54 [1:50:46<2:41:32, 302.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████▎     | 23/54 [1:55:48<2:36:22, 302.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████▍     | 24/54 [2:00:55<2:32:02, 304.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|████▋     | 25/54 [2:05:56<2:26:26, 302.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████▊     | 26/54 [2:10:59<2:21:30, 303.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 27/54 [2:16:01<2:16:17, 302.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 29/54 [2:26:03<2:05:46, 301.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████▌    | 30/54 [2:31:03<2:00:27, 301.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 31/54 [2:36:05<1:55:33, 301.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|█████▉    | 32/54 [2:41:06<1:50:26, 301.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|██████    | 33/54 [2:46:06<1:45:22, 301.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|██████▎   | 34/54 [2:51:11<1:40:42, 302.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▍   | 35/54 [2:56:12<1:35:35, 301.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 36/54 [3:01:13<1:30:26, 301.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|██████▊   | 37/54 [3:06:08<1:24:52, 299.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 39/54 [3:16:05<1:14:44, 298.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|███████▍  | 40/54 [3:21:09<1:10:05, 300.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|███████▌  | 41/54 [3:26:08<1:05:00, 300.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|███████▊  | 42/54 [3:31:09<1:00:02, 300.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████▉  | 43/54 [3:36:10<55:05, 300.46s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 45/54 [3:44:47<41:28, 276.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 46/54 [3:48:14<34:06, 255.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████▋ | 47/54 [3:51:34<27:53, 239.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|████████▉ | 48/54 [3:54:52<22:39, 226.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|█████████ | 49/54 [3:58:09<18:08, 217.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 50/54 [4:01:26<14:06, 211.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|█████████▍| 51/54 [4:04:44<10:22, 207.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████▋| 52/54 [4:08:01<06:48, 204.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|█████████▊| 53/54 [4:11:18<03:22, 202.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 54/54 [4:14:36<00:00, 200.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1,55)):\n",
    "#     days_from_next_promotion(i)\n",
    "#     days_from_prev_promotion(i)\n",
    "#     days_from_next_event(i)\n",
    "#     days_from_prev_event(i)\n",
    "#     days_from_next_holiday(i)\n",
    "#     days_from_prev_holiday(i)\n",
    "    days_from_next_nw_test(i)\n",
    "#     days_from_prev_nw(i)\n",
    "#     days_from_next_nn(i)\n",
    "#     days_from_prev_nn(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
